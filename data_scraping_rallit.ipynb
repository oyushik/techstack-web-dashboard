{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8708b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 로깅 설정 (노트북 출력 및 콘솔에 표시됨)\n",
    "# 기존 핸들러 제거 (노트북에서 재실행 시 중복 로깅 방지)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 요청 헤더 (실제 브라우저처럼 보이도록 설정)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Wanted-Platform': 'web', # 필요할 수 있는 헤더\n",
    "    'Wanted-Service': 'wanted', # 필요할 수 있는 헤더\n",
    "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7', # 언어 설정 추가\n",
    "}\n",
    "\n",
    "logging.info(\"라이브러리 임포트, 로깅 및 헤더 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10617f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직종 구분 선택\n",
    "job_category = \"total\" # \"total\" # \"backend\" # \"frontend\"\n",
    "\n",
    "if job_category == \"total\":\n",
    "    job_category_url = \"?jobGroup=DEVELOPER\"\n",
    "elif job_category == \"backend\":\n",
    "    job_category_url = \"?job=BACKEND_DEVELOPER&jobGroup=DEVELOPER\"\n",
    "elif job_category == \"frontend\":\n",
    "    job_category_url = \"?job=FRONTEND_DEVELOPER&jobGroup=DEVELOPER\"\n",
    "else:\n",
    "    job_category == \"total\"\n",
    "    job_category_url = \"?jobGroup=DEVELOPER\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크래핑 설정\n",
    "scraped_data = []\n",
    "start_page_no = 1\n",
    "\n",
    "base_url = \"https://www.rallit.com\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"스크래핑 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec41b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS 선택자를 이용하여 soup 객체에서 데이터를 추출하는 함수.\n",
    "def extract_data(soup, selector, attribute=None, is_list=False, default=\"N/A\"):\n",
    "    \"\"\"\n",
    "    주어진 CSS 선택자를 사용하여 BeautifulSoup 객체에서 데이터를 추출합니다.\n",
    "    Args:\n",
    "        soup (BeautifulSoup): 데이터를 추출할 BeautifulSoup 객체.\n",
    "        selector (str): CSS 선택자.\n",
    "        attribute (str, optional): 추출할 속성. None이면 텍스트 추출. Defaults to None.\n",
    "        is_list (bool, optional): 여러 개의 결과를 반환할지 여부. Defaults to False.\n",
    "        default (str, optional): 찾지 못했을 경우 반환할 기본값. Defaults to \"N/A\".\n",
    "\n",
    "    Returns:\n",
    "        str or list: 추출된 텍스트 또는 속성 값 (is_list=True인 경우 리스트).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_list:\n",
    "            elements = soup.select(selector)\n",
    "            if attribute:\n",
    "                return [element.get(attribute, default).strip() for element in elements]\n",
    "            else:\n",
    "                return [element.get_text(strip=True) for element in elements]\n",
    "        else:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                if attribute:\n",
    "                    return element.get(attribute, default).strip()\n",
    "                else:\n",
    "                    return element.get_text(strip=True)\n",
    "            else:\n",
    "                print(f\"Warning: Could not find element with selector '{selector}'.\")\n",
    "                return default\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data with selector '{selector}': {e}\")\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe567c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skill 데이터 필터링 함수 정의\n",
    "def filter_skill_data(skill):\n",
    "    \"\"\"\n",
    "    skill 데이터에서 조건부로 특수문자를 제거하고, 단어 목록 형태로 정리합니다.\n",
    "    \"\"\"\n",
    "    if not skill:\n",
    "        return \"\"\n",
    "\n",
    "    # 0. 한글 제거\n",
    "    no_hangul = re.compile('[ㄱ-ㅣ가-힣]+')\n",
    "    filtered_skill = no_hangul.sub('', skill)\n",
    "\n",
    "    # 1. 개행 문자 제거\n",
    "    filtered_skill = filtered_skill.replace('\\n', '')\n",
    "\n",
    "    # 2. LINE SEPARATOR 제거 (U+2028)\n",
    "    filtered_skill = filtered_skill.replace('\\u2028', '')\n",
    "\n",
    "    # # 3. 특수문자 중 '#', '+'를 제외하고 제거 (온점 포함)\n",
    "    # # 제거 대상: 알파벳, 숫자, '#', '+', 공백이 아닌 모든 문자 (온점 포함)\n",
    "    # filtered_skill = re.sub(r\"[^a-zA-Z0-9#+\\s]\", \"\", filtered_skill)\n",
    "    \n",
    "\n",
    "    # 제거 대상: 알파벳, '#', '+', 공백이 아닌 모든 문자 (온점 포함)\n",
    "    filtered_skill = re.sub(r\"[^a-zA-Z#+\\s]\", \"\", filtered_skill)\n",
    "\n",
    "\n",
    "    # 4. 알파벳 오른쪽 옆에 공백 없이 붙어 있는 숫자를 제외한 모든 숫자 제거\n",
    "    def remove_standalone_numbers(text):\n",
    "        def replace(match):\n",
    "            return \"\"\n",
    "        # 숫자 앞뒤로 알파벳이 없는 경우 제거\n",
    "        return re.sub(r\"(?<![a-zA-Z])\\d+(?![a-zA-Z])\", replace, text)\n",
    "\n",
    "    filtered_skill = remove_standalone_numbers(filtered_skill)\n",
    "\n",
    "    # 5. 단어 분리, 공백 제거, 중복 제거 및 ', '로 연결\n",
    "    words = filtered_skill.split()  # 공백을 기준으로 단어 분리\n",
    "    unique_words = []\n",
    "    seen = set()\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            unique_words.append(word)\n",
    "            seen.add(word)\n",
    "\n",
    "    filtered_skill = ', '.join(unique_words)\n",
    "\n",
    "    return filtered_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_no = start_page_no\n",
    "while True: # 무한 루프 시작 (종료 조건은 루프 내에 있음)\n",
    "    list_url = f\"{base_url}/{job_category_url}&pageNumber={page_no}\"\n",
    "    print(f\"Scraping page: {list_url}\")\n",
    "\n",
    "    try:\n",
    "        # 1. 채용 공고 목록 페이지 가져오기\n",
    "        response = requests.get(list_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 2. 각 채용 공고 컨테이너(article) 찾기\n",
    "        job_postings = soup.find_all('article')\n",
    "\n",
    "        # *** 동적 로딩 확인 ***\n",
    "        if not job_postings:\n",
    "            print(\"채용 공고를 찾을 수 없습니다.\")\n",
    "            break # 공고가 없으면 루프 종료\n",
    "        else:\n",
    "            print(f\"Found {len(job_postings)} job postings on page {page_no}.\")\n",
    "\n",
    "        # 3. 각 공고별 정보 추출\n",
    "        for item in job_postings:\n",
    "            company_name = \"N/A\"\n",
    "            position = \"N/A\"\n",
    "            skill = \"N/A\"\n",
    "\n",
    "            # 각 채용 공고 article 안의 div.css-vjt50z 컨테이너 찾기\n",
    "            job_info_container = item.find('div', class_='css-vjt50z')\n",
    "\n",
    "            if job_info_container:\n",
    "                # 3-1. 회사명 추출\n",
    "                company_name_element = job_info_container.find('p', class_='summary__company-name css-x5ccem')\n",
    "                if company_name_element:\n",
    "                    company_name = company_name_element.get_text(strip=True)\n",
    "\n",
    "                # 3-2. 공고 제목 추출\n",
    "                position_element = job_info_container.find('h3', class_='summary__title css-5g43jj')\n",
    "                if position_element:\n",
    "                    position = position_element.get_text(strip=True)\n",
    "\n",
    "                # 3-3. 상세 페이지 링크 추출 (이제 모든 skill 추출)\n",
    "                skill_elements = job_info_container.find_all('p', class_='css-13kyeyo')\n",
    "                skills_list = [skill.get_text(strip=True) for skill in skill_elements]\n",
    "                skill = ', '.join(skills_list) if skills_list else \"N/A\"\n",
    "\n",
    "                # --- 여기에 필터링 함수 적용 ---\n",
    "                filtered_skill = filter_skill_data(skill)\n",
    "                # --- 필터링 완료 ---\n",
    "\n",
    "                # 3-5. 결과 저장\n",
    "                scraped_data.append({\n",
    "                    'company': company_name,\n",
    "                    'position': position,\n",
    "                    'skill': filtered_skill,\n",
    "                })\n",
    "                print(\"    --- Scraped Data ---\")\n",
    "                print(f\"    company: {company_name}\")\n",
    "                print(f\"    position: {position}\")\n",
    "                print(f\"    skill: {filtered_skill[:100]}...\" if filtered_skill and len(filtered_skill) > 100 else filtered_skill if filtered_skill else \"N/A\") # 너무 길면 일부만 출력\n",
    "                print(\"    ----------------------\")\n",
    "            else:\n",
    "                print(\"    --- No job info container found ---\")\n",
    "\n",
    "        page_no += 1 # 다음 페이지로 이동\n",
    "        print(f\"Current page number: {page_no}\")\n",
    "        time.sleep(1) # 페이지 간 딜레이\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching list page {list_url}: {e}\")\n",
    "        break # 오류 발생 시 루프 종료\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "    # *** 종료 조건: 이번 페이지에서 개발자 공고 제목을 찾지 못한 경우 ***\n",
    "    if not soup.select_one('h3.summary__title.css-5g43jj'): # 개발자 공고 제목이 없다면 종료\n",
    "        print(\"No more developer job postings found. Stopping scraping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. CSV 파일 생성\n",
    "if not scraped_data:\n",
    "    logging.warning(\"수집된 데이터가 없습니다. CSV 파일을 생성하지 않습니다.\")\n",
    "else:\n",
    "    logging.info(\"수집된 데이터를 DataFrame으로 변환 중...\")\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    # CSV 파일을 저장할 폴더 이름\n",
    "    data_folder = 'data'\n",
    "\n",
    "    # CSV 파일 이름 설정\n",
    "    filename = f'data_rallit_{job_category}.csv'\n",
    "\n",
    "    # 저장할 전체 경로 생성\n",
    "    filepath = os.path.join(data_folder, filename)\n",
    "\n",
    "    # 해당 폴더가 없으면 생성\n",
    "    if not os.path.exists(data_folder):\n",
    "        try:\n",
    "            os.makedirs(data_folder)\n",
    "            logging.info(f\"'{data_folder}' 폴더를 생성했습니다.\")\n",
    "        except OSError as e:\n",
    "            logging.error(f\"'{data_folder}' 폴더 생성 중 오류 발생: {e}\", exc_info=True)\n",
    "            print(f\"\\n폴더 생성 실패: {e}\")\n",
    "    else:\n",
    "        logging.info(f\"'{data_folder}' 폴더가 이미 존재합니다.\")\n",
    "\n",
    "    try:\n",
    "        # encoding='utf-8-sig' : Excel에서 한글 깨짐 방지 (BOM 포함 UTF-8)\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "        logging.info(f\"DataFrame이 '{filepath}'으로 성공적으로 저장되었습니다.\")\n",
    "        print(f\"\\n파일 저장 완료: {filepath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"DataFrame을 CSV로 저장하는 중 오류 발생: {e}\", exc_info=True)\n",
    "        print(f\"\\n파일 저장 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a717271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_csv(f\"data/{filename}\", encoding='utf-8-sig')\n",
    "df_read"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
