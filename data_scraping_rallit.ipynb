{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8708b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import 및 logging 설정\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import data_utils\n",
    "\n",
    "data_utils.setup_logging()\n",
    "logging.info(\"필수 라이브러리 및 data_utils 모듈 임포트, 로깅 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10617f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직종 구분 선택 -> 사이트마다 다르게 설정할 것\n",
    "job_category = \"total\" # \"total\" # \"backend\" # \"frontend\"\n",
    "\n",
    "if job_category == \"total\":\n",
    "    job_category_url = \"?jobGroup=DEVELOPER\"\n",
    "elif job_category == \"backend\":\n",
    "    job_category_url = \"?job=BACKEND_DEVELOPER&jobGroup=DEVELOPER\"\n",
    "elif job_category == \"frontend\":\n",
    "    job_category_url = \"?job=FRONTEND_DEVELOPER&jobGroup=DEVELOPER\"\n",
    "else:\n",
    "    job_category == \"total\"\n",
    "    job_category_url = \"?jobGroup=DEVELOPER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace473fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크래핑 설정\n",
    "scraped_data = []\n",
    "start_page_no = 1\n",
    "\n",
    "# 아래 사이트 관련은 사이트 별로 변경 필수\n",
    "base_url = \"https://www.rallit.com\"\n",
    "site_name = 'rallit'\n",
    "\n",
    "headers = data_utils.DEFAULT_HEADERS\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"스크래핑 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS 선택자를 이용하여 soup 객체에서 데이터를 추출하는 함수.\n",
    "# 이 함수는 현재 스크립트(Notebook)에서 사용됩니다.\n",
    "def extract_data(soup, selector, attribute=None, is_list=False, default=\"N/A\"):\n",
    "    \"\"\"\n",
    "    주어진 CSS 선택자를 사용하여 BeautifulSoup 객체에서 데이터를 추출합니다.\n",
    "    Args:\n",
    "        soup (BeautifulSoup): 데이터를 추출할 BeautifulSoup 객체.\n",
    "        selector (str): CSS 선택자.\n",
    "        attribute (str, optional): 추출할 속성. None이면 텍스트 추출. Defaults to None.\n",
    "        is_list (bool, optional): 여러 개의 결과를 반환할지 여부. Defaults to False.\n",
    "        default (str, optional): 찾지 못했을 경우 반환할 기본값. Defaults to \"N/A\".\n",
    "\n",
    "    Returns:\n",
    "        str or list: 추출된 텍스트 또는 속성 값 (is_list=True인 경우 리스트).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_list:\n",
    "            elements = soup.select(selector)\n",
    "            if attribute:\n",
    "                return [element.get(attribute, default).strip() for element in elements]\n",
    "            else:\n",
    "                return [element.get_text(strip=True) for element in elements]\n",
    "        else:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                if attribute:\n",
    "                    return element.get(attribute, default).strip()\n",
    "                else:\n",
    "                    return element.get_text(strip=True)\n",
    "            else:\n",
    "                # print(f\"Warning: Could not find element with selector '{selector}'.\") # 로깅으로 대체 권장\n",
    "                logging.warning(f\"Could not find element with selector '{selector}'.\")\n",
    "                return default\n",
    "    except Exception as e:\n",
    "        # print(f\"Error extracting data with selector '{selector}': {e}\") # 로깅으로 대체 권장\n",
    "        logging.error(f\"Error extracting data with selector '{selector}': {e}\", exc_info=True)\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec41b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 스크래핑 루프 실행\n",
    "page_no = start_page_no\n",
    "while True: # 무한 루프 시작 (종료 조건은 루프 내에 있음)\n",
    "    # 대상 URL 구성\n",
    "    list_url = f\"{base_url}{job_category_url}&pageNumber={page_no}\"\n",
    "    logging.info(f\"Scraping page: {list_url}\")\n",
    "\n",
    "    try:\n",
    "        # 1. 채용 공고 목록 페이지 가져오기\n",
    "        response = requests.get(list_url, headers=headers)\n",
    "        response.raise_for_status() # HTTP 오류 발생 시 예외 발생\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 2. 각 채용 공고 컨테이너(article) 찾기\n",
    "        job_postings = soup.find_all('article')\n",
    "\n",
    "        # *** 동적 로딩 확인 및 루프 종료 조건 ***\n",
    "        # 페이지에 공고가 없거나, 개발자 공고 제목 요소를 찾지 못하면 종료\n",
    "        if not job_postings or not soup.select_one('h3.summary__title.css-5g43jj'):\n",
    "            if not job_postings:\n",
    "                logging.info(f\"Page {page_no}: 채용 공고 article 태그를 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "                print(\"\\n채용 공고 article 태그를 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "            else: # job_postings는 있지만, 특정 css selector가 없는 경우\n",
    "                logging.info(f\"Page {page_no}: 더 이상 개발자 공고 제목(h3.summary__title.css-5g43jj)을 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "                print(\"\\n더 이상 개발자 공고 제목을 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "            break # 공고가 없거나 종료 조건 만족 시 루프 종료\n",
    "        else:\n",
    "            logging.info(f\"Found {len(job_postings)} job postings on page {page_no}.\")\n",
    "            print(f\"Found {len(job_postings)} job postings on page {page_no}.\")\n",
    "\n",
    "\n",
    "        # 3. 각 공고별 정보 추출\n",
    "        for item in job_postings:\n",
    "            company_name = \"N/A\"\n",
    "            position = \"N/A\"\n",
    "            skill = \"N/A\"\n",
    "\n",
    "            # 각 채용 공고 article 안의 div.css-vjt50z 컨테이너 찾기\n",
    "            job_info_container = item.find('div', class_='css-vjt50z')\n",
    "\n",
    "            if job_info_container:\n",
    "                # 3-1. 회사명 추출\n",
    "                company_name_element = job_info_container.find('p', class_='summary__company-name css-x5ccem')\n",
    "                if company_name_element:\n",
    "                    company_name = company_name_element.get_text(strip=True)\n",
    "\n",
    "                # 3-2. 공고 제목 추출\n",
    "                position_element = job_info_container.find('h3', class_='summary__title css-5g43jj')\n",
    "                if position_element:\n",
    "                    position = position_element.get_text(strip=True)\n",
    "\n",
    "                # 3-3. 상세 페이지 링크 추출 (모든 skill 추출)\n",
    "                skill_elements = job_info_container.find_all('p', class_='css-13kyeyo')\n",
    "                skills_list = [skill_el.get_text(strip=True) for skill_el in skill_elements] # 변수명 충돌 피함\n",
    "                skill = ', '.join(skills_list) if skills_list else \"N/A\"\n",
    "\n",
    "                # --- 여기에 filtering_utils 모듈의 필터링 함수 적용 ---\n",
    "                filtered_skill = data_utils.filter_skill_data(skill)\n",
    "                # --- 필터링 완료 ---\n",
    "\n",
    "                # 3-5. 결과 저장\n",
    "                scraped_data.append({\n",
    "                    'company': company_name,\n",
    "                    'position': position,\n",
    "                    'skill': filtered_skill,\n",
    "                })\n",
    "                # 상세 로그는 너무 길 수 있으니 필요에 따라 주석 처리하거나 레벨 조정\n",
    "                # logging.debug(\"    --- Scraped Data ---\")\n",
    "                # logging.debug(f\"    company: {company_name}\")\n",
    "                # logging.debug(f\"    position: {position}\")\n",
    "                # logging.debug(f\"    skill: {filtered_skill[:100]}...\" if filtered_skill and len(filtered_skill) > 100 else filtered_skill if filtered_skill else \"N/A\")\n",
    "                # logging.debug(\"    ----------------------\")\n",
    "            else:\n",
    "                logging.warning(\"    --- No job info container found for an article element ---\")\n",
    "                # print(\"    --- No job info container found ---\") # 로깅으로 대체 권장\n",
    "\n",
    "        page_no += 1 # 다음 페이지로 이동\n",
    "        print(f\"Current page number: {page_no}\")\n",
    "        time.sleep(1) # 페이지 간 딜레이 (필수)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching list page {list_url}: {e}\", exc_info=True)\n",
    "        print(f\"\\nError fetching list page {list_url}: {e}\")\n",
    "        break # 네트워크 또는 HTTP 오류 발생 시 루프 종료\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during scraping: {e}\", exc_info=True)\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "logging.info(\"스크래핑 루프 종료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_utils 모듈의 save_data_to_csv 함수를 사용하여 데이터를 저장합니다.\n",
    "data_folder = 'data'\n",
    "filename = f'data_{site_name}_{job_category}.csv'\n",
    "\n",
    "saved_filepath = data_utils.save_data_to_csv(scraped_data, filename, folder=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a717271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 CSV 파일을 읽어와 DataFrame으로 출력 (확인용)\n",
    "data_utils.load_data_from_csv(saved_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
