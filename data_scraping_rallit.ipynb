{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8708b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:45,203 - INFO - 로깅 설정 완료\n",
      "2025-04-19 00:19:45,203 - INFO - 필수 라이브러리 및 data_utils 모듈 임포트, 로깅 설정 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# 자체 제작 유틸리티 모듈 임포트\n",
    "import data_utils\n",
    "\n",
    "# 로깅 설정\n",
    "# data_utils 모듈의 setup_logging 함수를 호출하여 로깅을 설정합니다.\n",
    "# 기존 핸들러 제거 및 basicConfig 설정은 이 함수 안에서 처리됩니다.\n",
    "data_utils.setup_logging()\n",
    "logging.info(\"필수 라이브러리 및 data_utils 모듈 임포트, 로깅 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10617f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:45,217 - INFO - 스크래핑 설정 완료.\n"
     ]
    }
   ],
   "source": [
    "# 직종 구분 선택 -> 사이트마다 다르게 설정할 것\n",
    "job_category = \"total\" # \"total\" # \"backend\" # \"frontend\"\n",
    "\n",
    "if job_category == \"total\":\n",
    "    job_category_url = \"?jobGroup=DEVELOPER\"\n",
    "elif job_category == \"backend\":\n",
    "    job_category_url = \"?job=BACKEND_DEVELOPER&jobGroup=DEVELOPER\"\n",
    "elif job_category == \"frontend\":\n",
    "    job_category_url = \"?job=FRONTEND_DEVELOPER&jobGroup=DEVELOPER\"\n",
    "else:\n",
    "    job_category == \"total\"\n",
    "    job_category_url = \"?jobGroup=DEVELOPER\"\n",
    "\n",
    "scraped_data = []\n",
    "start_page_no = 1\n",
    "\n",
    "base_url = \"https://www.rallit.com\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"스크래핑 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06b3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀3\n",
    "# CSS 선택자를 이용하여 soup 객체에서 데이터를 추출하는 함수.\n",
    "# 이 함수는 현재 스크립트(Notebook)에서 사용됩니다.\n",
    "def extract_data(soup, selector, attribute=None, is_list=False, default=\"N/A\"):\n",
    "    \"\"\"\n",
    "    주어진 CSS 선택자를 사용하여 BeautifulSoup 객체에서 데이터를 추출합니다.\n",
    "    Args:\n",
    "        soup (BeautifulSoup): 데이터를 추출할 BeautifulSoup 객체.\n",
    "        selector (str): CSS 선택자.\n",
    "        attribute (str, optional): 추출할 속성. None이면 텍스트 추출. Defaults to None.\n",
    "        is_list (bool, optional): 여러 개의 결과를 반환할지 여부. Defaults to False.\n",
    "        default (str, optional): 찾지 못했을 경우 반환할 기본값. Defaults to \"N/A\".\n",
    "\n",
    "    Returns:\n",
    "        str or list: 추출된 텍스트 또는 속성 값 (is_list=True인 경우 리스트).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_list:\n",
    "            elements = soup.select(selector)\n",
    "            if attribute:\n",
    "                return [element.get(attribute, default).strip() for element in elements]\n",
    "            else:\n",
    "                return [element.get_text(strip=True) for element in elements]\n",
    "        else:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                if attribute:\n",
    "                    return element.get(attribute, default).strip()\n",
    "                else:\n",
    "                    return element.get_text(strip=True)\n",
    "            else:\n",
    "                # print(f\"Warning: Could not find element with selector '{selector}'.\") # 로깅으로 대체 권장\n",
    "                logging.warning(f\"Could not find element with selector '{selector}'.\")\n",
    "                return default\n",
    "    except Exception as e:\n",
    "        # print(f\"Error extracting data with selector '{selector}': {e}\") # 로깅으로 대체 권장\n",
    "        logging.error(f\"Error extracting data with selector '{selector}': {e}\", exc_info=True)\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec41b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:45,246 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=1\n",
      "2025-04-19 00:19:45,512 - INFO - Found 20 job postings on page 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 1.\n",
      "Current page number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:46,516 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=2\n",
      "2025-04-19 00:19:46,686 - INFO - Found 20 job postings on page 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 2.\n",
      "Current page number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:47,691 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=3\n",
      "2025-04-19 00:19:47,948 - INFO - Found 20 job postings on page 3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 3.\n",
      "Current page number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:48,952 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=4\n",
      "2025-04-19 00:19:49,121 - INFO - Found 20 job postings on page 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 4.\n",
      "Current page number: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:50,131 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=5\n",
      "2025-04-19 00:19:50,344 - INFO - Found 20 job postings on page 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 5.\n",
      "Current page number: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:51,347 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=6\n",
      "2025-04-19 00:19:51,527 - INFO - Found 20 job postings on page 6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 6.\n",
      "Current page number: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:52,534 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=7\n",
      "2025-04-19 00:19:52,696 - INFO - Found 20 job postings on page 7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 7.\n",
      "Current page number: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:53,699 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=8\n",
      "2025-04-19 00:19:53,904 - INFO - Found 20 job postings on page 8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 8.\n",
      "Current page number: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:54,916 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=9\n",
      "2025-04-19 00:19:55,087 - INFO - Found 20 job postings on page 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 9.\n",
      "Current page number: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:56,095 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=10\n",
      "2025-04-19 00:19:56,294 - INFO - Found 20 job postings on page 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 10.\n",
      "Current page number: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:57,298 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=11\n",
      "2025-04-19 00:19:57,468 - INFO - Found 20 job postings on page 11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 11.\n",
      "Current page number: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:58,474 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=12\n",
      "2025-04-19 00:19:58,738 - INFO - Found 20 job postings on page 12.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 12.\n",
      "Current page number: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:19:59,741 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=13\n",
      "2025-04-19 00:19:59,924 - INFO - Found 20 job postings on page 13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 13.\n",
      "Current page number: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:20:00,928 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=14\n",
      "2025-04-19 00:20:01,091 - INFO - Found 20 job postings on page 14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 14.\n",
      "Current page number: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:20:02,095 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=15\n",
      "2025-04-19 00:20:02,286 - INFO - Found 20 job postings on page 15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings on page 15.\n",
      "Current page number: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:20:03,290 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=16\n",
      "2025-04-19 00:20:03,445 - INFO - Found 12 job postings on page 16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 job postings on page 16.\n",
      "Current page number: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:20:04,448 - INFO - Scraping page: https://www.rallit.com?jobGroup=DEVELOPER&pageNumber=17\n",
      "2025-04-19 00:20:04,571 - INFO - Page 17: 채용 공고 article 태그를 찾을 수 없습니다. 스크래핑을 종료합니다.\n",
      "2025-04-19 00:20:04,571 - INFO - 스크래핑 루프 종료.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "채용 공고 article 태그를 찾을 수 없습니다. 스크래핑을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "# 셀4\n",
    "# 실제 스크래핑 루프 실행\n",
    "page_no = start_page_no\n",
    "while True: # 무한 루프 시작 (종료 조건은 루프 내에 있음)\n",
    "    # 대상 URL 구성\n",
    "    list_url = f\"{base_url}{job_category_url}&pageNumber={page_no}\"\n",
    "    logging.info(f\"Scraping page: {list_url}\")\n",
    "\n",
    "    try:\n",
    "        # 1. 채용 공고 목록 페이지 가져오기\n",
    "        # data_utils.DEFAULT_HEADERS 상수를 사용합니다.\n",
    "        response = requests.get(list_url, headers=data_utils.DEFAULT_HEADERS)\n",
    "        response.raise_for_status() # HTTP 오류 발생 시 예외 발생\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 2. 각 채용 공고 컨테이너(article) 찾기\n",
    "        job_postings = soup.find_all('article')\n",
    "\n",
    "        # *** 동적 로딩 확인 및 루프 종료 조건 ***\n",
    "        # 페이지에 공고가 없거나, 개발자 공고 제목 요소를 찾지 못하면 종료\n",
    "        if not job_postings or not soup.select_one('h3.summary__title.css-5g43jj'):\n",
    "            if not job_postings:\n",
    "                logging.info(f\"Page {page_no}: 채용 공고 article 태그를 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "                print(\"\\n채용 공고 article 태그를 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "            else: # job_postings는 있지만, 특정 css selector가 없는 경우\n",
    "                logging.info(f\"Page {page_no}: 더 이상 개발자 공고 제목(h3.summary__title.css-5g43jj)을 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "                print(\"\\n더 이상 개발자 공고 제목을 찾을 수 없습니다. 스크래핑을 종료합니다.\")\n",
    "            break # 공고가 없거나 종료 조건 만족 시 루프 종료\n",
    "        else:\n",
    "            logging.info(f\"Found {len(job_postings)} job postings on page {page_no}.\")\n",
    "            print(f\"Found {len(job_postings)} job postings on page {page_no}.\")\n",
    "\n",
    "\n",
    "        # 3. 각 공고별 정보 추출\n",
    "        for item in job_postings:\n",
    "            company_name = \"N/A\"\n",
    "            position = \"N/A\"\n",
    "            skill = \"N/A\"\n",
    "\n",
    "            # 각 채용 공고 article 안의 div.css-vjt50z 컨테이너 찾기\n",
    "            job_info_container = item.find('div', class_='css-vjt50z')\n",
    "\n",
    "            if job_info_container:\n",
    "                # 3-1. 회사명 추출\n",
    "                company_name_element = job_info_container.find('p', class_='summary__company-name css-x5ccem')\n",
    "                if company_name_element:\n",
    "                    company_name = company_name_element.get_text(strip=True)\n",
    "\n",
    "                # 3-2. 공고 제목 추출\n",
    "                position_element = job_info_container.find('h3', class_='summary__title css-5g43jj')\n",
    "                if position_element:\n",
    "                    position = position_element.get_text(strip=True)\n",
    "\n",
    "                # 3-3. 상세 페이지 링크 추출 (모든 skill 추출)\n",
    "                skill_elements = job_info_container.find_all('p', class_='css-13kyeyo')\n",
    "                skills_list = [skill_el.get_text(strip=True) for skill_el in skill_elements] # 변수명 충돌 피함\n",
    "                skill = ', '.join(skills_list) if skills_list else \"N/A\"\n",
    "\n",
    "                # --- 여기에 filtering_utils 모듈의 필터링 함수 적용 ---\n",
    "                filtered_skill = data_utils.filter_skill_data(skill)\n",
    "                # --- 필터링 완료 ---\n",
    "\n",
    "                # 3-5. 결과 저장\n",
    "                scraped_data.append({\n",
    "                    'company': company_name,\n",
    "                    'position': position,\n",
    "                    'skill': filtered_skill,\n",
    "                })\n",
    "                # 상세 로그는 너무 길 수 있으니 필요에 따라 주석 처리하거나 레벨 조정\n",
    "                # logging.debug(\"    --- Scraped Data ---\")\n",
    "                # logging.debug(f\"    company: {company_name}\")\n",
    "                # logging.debug(f\"    position: {position}\")\n",
    "                # logging.debug(f\"    skill: {filtered_skill[:100]}...\" if filtered_skill and len(filtered_skill) > 100 else filtered_skill if filtered_skill else \"N/A\")\n",
    "                # logging.debug(\"    ----------------------\")\n",
    "            else:\n",
    "                logging.warning(\"    --- No job info container found for an article element ---\")\n",
    "                # print(\"    --- No job info container found ---\") # 로깅으로 대체 권장\n",
    "\n",
    "        page_no += 1 # 다음 페이지로 이동\n",
    "        print(f\"Current page number: {page_no}\")\n",
    "        time.sleep(1) # 페이지 간 딜레이 (필수)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching list page {list_url}: {e}\", exc_info=True)\n",
    "        print(f\"\\nError fetching list page {list_url}: {e}\")\n",
    "        break # 네트워크 또는 HTTP 오류 발생 시 루프 종료\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during scraping: {e}\", exc_info=True)\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "logging.info(\"스크래핑 루프 종료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502a8413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:20:04,580 - INFO - 수집된 데이터를 DataFrame으로 변환 중...\n",
      "2025-04-19 00:20:04,582 - INFO - 'data' 폴더가 이미 존재합니다.\n",
      "2025-04-19 00:20:04,590 - INFO - DataFrame이 'data\\temp.csv'으로 성공적으로 저장되었습니다.\n",
      "2025-04-19 00:20:04,591 - INFO - 데이터 저장 과정 완료. 파일 경로: data\\temp.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "파일 저장 완료: data\\temp.csv\n"
     ]
    }
   ],
   "source": [
    "# 셀5\n",
    "# CSV 파일 저장\n",
    "# CSV 파일 이름을 설정합니다. 이 이름은 이 Notebook 실행에 고유합니다.\n",
    "data_folder = 'data'\n",
    "# filename = f'data_rallit_{job_category}.csv'\n",
    "filename = 'temp.csv'\n",
    "\n",
    "# data_utils 모듈의 save_data_to_csv 함수를 사용하여 데이터를 저장합니다.\n",
    "# 폴더 생성, DataFrame 변환 및 CSV 저장 로직은 함수 안에서 처리됩니다.\n",
    "saved_filepath = data_utils.save_data_to_csv(scraped_data, filename, folder=data_folder)\n",
    "\n",
    "if saved_filepath:\n",
    "    logging.info(f\"데이터 저장 과정 완료. 파일 경로: {saved_filepath}\")\n",
    "else:\n",
    "    logging.warning(\"데이터 저장에 실패했거나 저장할 데이터가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a717271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:20:04,620 - INFO - 'data\\temp.csv' 파일에서 DataFrame 로드 성공.\n",
      "2025-04-19 00:20:04,620 - INFO - 저장된 CSV 파일 DataFrame 출력.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 로드된 데이터프레임 (처음 5행) ---\n",
      "    company                                    position  \\\n",
      "0      오비맥주  [ABInBev Korea 오비맥주] BEES Product Engineer   \n",
      "1       와디즈            [와디즈] 자바 백엔드 개발 (병역특례/전문연구요원/전직)   \n",
      "2  이지식스(엠블)                          [TADA] QA Engineer   \n",
      "3  CLASS101                                  소프트웨어 엔지니어   \n",
      "4    (주)야나두                     [야나두(야핏)] 안드로이드(AOS) 개발   \n",
      "\n",
      "                                               skill  \n",
      "0                                             Access  \n",
      "1            Java, Spring, Boot, REST, API, AWS, Git  \n",
      "2                                       JIRA, Github  \n",
      "3  Java, Kotlin, Spring, Boot, MongoDB, AWS, Kube...  \n",
      "4                        Kotlin, Java, MVVM, Jetpack  \n",
      "\n",
      "총 로드된 행 수: 312\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 셀6\n",
    "# 저장된 CSV 파일을 읽어와 DataFrame으로 출력 (확인용)\n",
    "# 저장된 파일 경로를 사용하거나, filename과 data_folder를 조합하여 사용합니다.\n",
    "# save_data_to_csv 함수가 성공 시 파일 경로를 반환하므로 그것을 사용합니다.\n",
    "if saved_filepath:\n",
    "    # data_utils 모듈의 load_data_from_csv 함수를 사용하여 파일을 읽어옵니다.\n",
    "    df_read = data_utils.load_data_from_csv(saved_filepath)\n",
    "\n",
    "    if df_read is not None:\n",
    "        logging.info(\"저장된 CSV 파일 DataFrame 출력.\")\n",
    "        print(\"\\n--- 로드된 데이터프레임 (처음 5행) ---\")\n",
    "        print(df_read.head())\n",
    "        print(f\"\\n총 로드된 행 수: {len(df_read)}\")\n",
    "        print(\"------------------------------------\")\n",
    "    else:\n",
    "        logging.warning(\"CSV 파일을 로드하지 못했습니다.\")\n",
    "else:\n",
    "    logging.warning(\"저장된 파일 경로가 없어 CSV 로드 테스트를 건너뜁니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
