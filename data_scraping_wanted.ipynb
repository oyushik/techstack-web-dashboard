{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 로깅 설정 (노트북 출력 및 콘솔에 표시됨)\n",
    "# 기존 핸들러 제거 (노트북에서 재실행 시 중복 로깅 방지)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 요청 헤더 (실제 브라우저처럼 보이도록 설정)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Wanted-Platform': 'web', # 필요할 수 있는 헤더\n",
    "    'Wanted-Service': 'wanted', # 필요할 수 있는 헤더\n",
    "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7', # 언어 설정 추가\n",
    "}\n",
    "\n",
    "logging.info(\"라이브러리 임포트, 로깅 및 헤더 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e979d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직종 구분 선택\n",
    "job_category = \"total\" # \"total\" # \"backend\" # \"frontend\"\n",
    "\n",
    "if job_category == \"total\":\n",
    "    job_category_url = \"?\"\n",
    "elif job_category == \"backend\":\n",
    "    job_category_url = \"?job_ids=872\"\n",
    "elif job_category == \"frontend\":\n",
    "    job_category_url = \"?job_ids=669\"\n",
    "else:\n",
    "    job_category_url = \"?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크래핑 설정\n",
    "scraped_data = []\n",
    "offset_no = 0\n",
    "\n",
    "# limit = 페이지 당 가져올 공고 수\n",
    "# max_offset = 최대 가져올 공고 수\n",
    "limit = 20\n",
    "max_offset = 600\n",
    "\n",
    "api_base_url = f\"https://www.wanted.co.kr/api/chaos/navigation/v1/results{job_category_url}\"\n",
    "\n",
    "# 기본 URL 파라미터\n",
    "base_params = {\n",
    "    'job_group_id': 518, # 개발 직군 전체\n",
    "    'country': 'kr',\n",
    "    'job_sort': 'job.latest_order', # 최신순 정렬\n",
    "    'years': -1, # 경력 무관\n",
    "    'locations': 'all', # 전체 지역\n",
    "    'limit': limit,\n",
    "    # 'offset'은 루프 내에서 설정됨\n",
    "}\n",
    "\n",
    "logging.info(f\"스크래핑 설정 완료. 최대 offset: {max_offset}, 페이지당 공고 수: {limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c33810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상세 정보 추출 함수 정의\n",
    "def get_job_details(job_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    주어진 job_id에 해당하는 상세 페이지 API에서 직무 상세 설명을 추출합니다.\n",
    "    API 엔드포인트: https://www.wanted.co.kr/api/chaos/jobs/v4/{job_id}/details\n",
    "    추출 필드: data.job.detail.main_tasks, data.job.detail.requirements, data.job.detail.preferred_points\n",
    "    \"\"\"\n",
    "\n",
    "    api_url = f\"https://www.wanted.co.kr/api/chaos/jobs/v4/{job_id}/details\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()  # HTTP 오류 발생 시 예외 발생\n",
    "        data = response.json()\n",
    "\n",
    "        if 'data' in data and 'job' in data['data'] and 'detail' in data['data']['job']:\n",
    "            detail = data['data']['job']['detail']\n",
    "            main_tasks = detail.get('main_tasks', '')\n",
    "            requirements = detail.get('requirements', '')\n",
    "            preferred_points = detail.get('preferred_points', '')\n",
    "\n",
    "            # 각 항목을 줄바꿈 두 번으로 연결하여 상세 정보 생성\n",
    "            detail_text = \"\\n\\n\".join(filter(None, [main_tasks, requirements, preferred_points])).strip()\n",
    "            return detail_text\n",
    "        else:\n",
    "            logging.warning(f\"Detail section not found in API response for job ID {job_id}. URL: {api_url}. Response: {data}\")\n",
    "            return \"상세 내용 없음 (API 응답에 Detail 섹션 부재)\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching details from API for job ID {job_id}: {e}. URL: {api_url}\")\n",
    "        return f\"상세 내용 로드 실패 (API 요청 오류: {e})\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing API response for job ID {job_id}: {e}\", exc_info=True)\n",
    "        return f\"상세 내용 파싱 실패 (API 응답 오류: {e})\"\n",
    "\n",
    "# # --- 사용 예시 (테스트용, 실제 스크래핑 루프에서는 호출됨) ---\n",
    "# test_job_id = 260432 # 테스트하려는 실제 공고 ID 입력\n",
    "# test_details = get_job_details(test_job_id)\n",
    "# print(f\"--- 테스트 결과 (Job ID: {test_job_id}) ---\")\n",
    "# print(test_details)\n",
    "# print(type(get_job_details(test_job_id)))\n",
    "# print(\"--- 테스트 종료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d97a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skill 데이터 필터링 함수 정의\n",
    "def filter_skill_data(skill):\n",
    "    \"\"\"\n",
    "    skill 데이터에서 조건부로 특수문자를 제거하고, 단어 목록 형태로 정리합니다.\n",
    "    \"\"\"\n",
    "    if not skill:\n",
    "        return \"\"\n",
    "\n",
    "    # 0. 한글 제거\n",
    "    no_hangul = re.compile('[ㄱ-ㅣ가-힣]+')\n",
    "    filtered_skill = no_hangul.sub('', skill)\n",
    "\n",
    "    # 1. 개행 문자 제거\n",
    "    filtered_skill = filtered_skill.replace('\\n', '')\n",
    "\n",
    "    # 2. LINE SEPARATOR 제거 (U+2028)\n",
    "    filtered_skill = filtered_skill.replace('\\u2028', '')\n",
    "\n",
    "    # 3. 특수문자 중 '#', '+'를 제외하고 제거 (온점 포함)\n",
    "    # 제거 대상: 알파벳, 숫자, '#', '+', 공백이 아닌 모든 문자 (온점 포함)\n",
    "    filtered_skill = re.sub(r\"[^a-zA-Z0-9#+\\s]\", \"\", filtered_skill)\n",
    "\n",
    "    # 4. 알파벳 오른쪽 옆에 공백 없이 붙어 있는 숫자를 제외한 모든 숫자 제거\n",
    "    def remove_standalone_numbers(text):\n",
    "        def replace(match):\n",
    "            return \"\"\n",
    "        # 숫자 앞뒤로 알파벳이 없는 경우 제거\n",
    "        return re.sub(r\"(?<![a-zA-Z])\\d+(?![a-zA-Z])\", replace, text)\n",
    "\n",
    "    filtered_skill = remove_standalone_numbers(filtered_skill)\n",
    "\n",
    "    # 5. 단어 분리, 공백 제거, 중복 제거 및 ', '로 연결\n",
    "    words = filtered_skill.split()  # 공백을 기준으로 단어 분리\n",
    "    unique_words = []\n",
    "    seen = set()\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            unique_words.append(word)\n",
    "            seen.add(word)\n",
    "\n",
    "    filtered_skill = ', '.join(unique_words)\n",
    "\n",
    "    return filtered_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7be5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크래핑 루프\n",
    "logging.info(\"스크래핑 루프 시작...\")\n",
    "\n",
    "while offset_no < max_offset:\n",
    "    # 현재 offset을 포함한 파라미터 설정\n",
    "    current_params = base_params.copy()\n",
    "    current_params['offset'] = offset_no\n",
    "\n",
    "    logging.info(f\"--- Offset {offset_no} 데이터 가져오는 중 ---\")\n",
    "\n",
    "    try:\n",
    "        # API 요청 (타임아웃 20초)\n",
    "        response = requests.get(api_base_url, params=current_params, headers=headers, timeout=20)\n",
    "        response.raise_for_status() # 오류 발생 시 예외 발생\n",
    "        json_data = response.json()\n",
    "\n",
    "        if 'data' in json_data and json_data['data']:\n",
    "            jobs = json_data['data']\n",
    "            logging.info(f\"Offset {offset_no}: {len(jobs)}개 공고 발견.\")\n",
    "            #\n",
    "\n",
    "            for job in jobs:\n",
    "                try:\n",
    "                    company = job.get('company', {})\n",
    "                    company_id = job.get('id')\n",
    "                    company_name = company.get('name')\n",
    "                    position = job.get('position')\n",
    "                    # address = job.get('address', {}) # address 정보 가져오기\n",
    "                    # location = address.get('location') # location 정보 추출\n",
    "                    # district = address.get('district') # district 정보 추출\n",
    "    \n",
    "                    if not company_id:\n",
    "                        logging.warning(f\"ID가 없는 공고 건너<0xEB><0x9B><0x9C>. Position: {position}, Company: {company_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 상세 정보 가져오기 (정의된 함수 호출)\n",
    "                    logging.info(f\"  - 상세 정보 가져오는 중 (현재 offset: {offset_no}, ID: {company_id}, Company: {company_name})\") # 너무 길면 잘라서 로깅\n",
    "                    skill = get_job_details(company_id)\n",
    "                    # 각 상세 페이지 요청 사이에 1초 지연 (서버 부하 감소)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    # --- skill 값 필터링 (문자열 등 정규화) ---\n",
    "                    filtered_skill = filter_skill_data(skill) # 필터링 함수 호출\n",
    "                    # --- skill 값 필터링 완료 ---\n",
    "\n",
    "                    job_info = {\n",
    "                        'company': company_name,\n",
    "                        'position': position,\n",
    "                        'skill': filtered_skill,\n",
    "                        # 'job_id': company_id, # 참고용으로 ID도 저장\n",
    "                        # 'location': location, # location 정보 추가\n",
    "                        # 'district': district # district 정보 추가\n",
    "                    }\n",
    "                    scraped_data.append(job_info)\n",
    "    \n",
    "                except Exception as e:\n",
    "                    # 개별 공고 처리 중 오류 발생 시 로깅하고 계속 진행\n",
    "                    logging.error(f\"개별 공고 처리 중 오류 발생 (ID: {job.get('id')}): {e}\", exc_info=True) # 상세 오류 스택 출력\n",
    "                    continue\n",
    "\n",
    "            # 다음 페이지로 이동\n",
    "            offset_no += limit\n",
    "            # API 목록 요청 사이에 1.5초 지연\n",
    "            logging.info(f\"Offset {offset_no - limit} 처리 완료. 1.5초 대기...\")\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        else:\n",
    "            # API 응답에 'data'가 없거나 비어있는 경우\n",
    "            logging.info(f\"Offset {offset_no}에서 더 이상 데이터가 없거나 API 응답 구조 변경됨. 스크래핑 중단.\")\n",
    "            break\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.error(f\"API 요청 시간 초과 (Offset: {offset_no}). 10초 후 재시도...\")\n",
    "        time.sleep(10)\n",
    "        # continue # 현재 offset 재시도 위해 offset_no 증가시키지 않음 (루프 시작 시 재시도됨)\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logging.error(f\"HTTP 오류 발생 (Offset: {offset_no}): {e.response.status_code} - {e.response.reason}\")\n",
    "        if e.response.status_code == 429: # Too Many Requests\n",
    "            logging.warning(\"요청 속도 제한(429) 감지됨. 60초 대기 후 재시도...\")\n",
    "            time.sleep(60)\n",
    "            # continue # 재시도\n",
    "        elif 400 <= e.response.status_code < 500:\n",
    "            logging.error(\"클라이언트 오류 발생. 요청 파라미터나 헤더 확인 필요. 스크래핑 중단.\")\n",
    "            break\n",
    "        elif e.response.status_code >= 500:\n",
    "            logging.warning(\"서버 오류 발생. 30초 대기 후 재시도...\")\n",
    "            time.sleep(30)\n",
    "            # continue # 재시도\n",
    "        else:\n",
    "            logging.error(\"알 수 없는 HTTP 오류. 스크래핑 중단.\")\n",
    "            break\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"API 요청 중 오류 발생 (Offset: {offset_no}): {e}\")\n",
    "        logging.info(\"네트워크 연결 확인 후 20초 대기...\")\n",
    "        time.sleep(20)\n",
    "        # continue # 재시도\n",
    "\n",
    "    except Exception as e:\n",
    "        # 기타 예외 처리 (JSON 파싱 오류 등)\n",
    "        logging.error(f\"스크래핑 루프 중 예기치 않은 오류 발생 (Offset: {offset_no}): {e}\", exc_info=True)\n",
    "        logging.info(\"오류로 인해 스크래핑 중단.\")\n",
    "        break\n",
    "\n",
    "logging.info(f\"스크래핑 루프 종료. 총 {len(scraped_data)}개의 공고 데이터 수집 완료.\")\n",
    "\n",
    "# # (선택적) 수집된 데이터 개수 확인\n",
    "# print(f\"Total jobs collected: {len(scraped_data)}\")\n",
    "# # (선택적) 첫번째 데이터 샘플 확인\n",
    "# if scraped_data:\n",
    "#     print(\"\\nSample Data:\")\n",
    "#     print(scraped_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 생성\n",
    "if not scraped_data:\n",
    "    logging.warning(\"수집된 데이터가 없습니다. CSV 파일을 생성하지 않습니다.\")\n",
    "else:\n",
    "    logging.info(\"수집된 데이터를 DataFrame으로 변환 중...\")\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    # CSV 파일을 저장할 폴더 이름\n",
    "    data_folder = 'data'\n",
    "\n",
    "    # CSV 파일 이름 설정\n",
    "    filename = f'data_wanted_{job_category}.csv'\n",
    "\n",
    "    # 저장할 전체 경로 생성\n",
    "    filepath = os.path.join(data_folder, filename)\n",
    "\n",
    "    # 해당 폴더가 없으면 생성\n",
    "    if not os.path.exists(data_folder):\n",
    "        try:\n",
    "            os.makedirs(data_folder)\n",
    "            logging.info(f\"'{data_folder}' 폴더를 생성했습니다.\")\n",
    "        except OSError as e:\n",
    "            logging.error(f\"'{data_folder}' 폴더 생성 중 오류 발생: {e}\", exc_info=True)\n",
    "            print(f\"\\n폴더 생성 실패: {e}\")\n",
    "    else:\n",
    "        logging.info(f\"'{data_folder}' 폴더가 이미 존재합니다.\")\n",
    "\n",
    "    try:\n",
    "        # encoding='utf-8-sig' : Excel에서 한글 깨짐 방지 (BOM 포함 UTF-8)\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "        logging.info(f\"DataFrame이 '{filepath}'으로 성공적으로 저장되었습니다.\")\n",
    "        print(f\"\\n파일 저장 완료: {filepath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"DataFrame을 CSV로 저장하는 중 오류 발생: {e}\", exc_info=True)\n",
    "        print(f\"\\n파일 저장 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246be554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_csv(f\"data/{filename}\", encoding='utf-8-sig')\n",
    "df_read"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
