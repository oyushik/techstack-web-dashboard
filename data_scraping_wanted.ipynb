{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import 및 logging 설정\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import data_utils\n",
    "\n",
    "data_utils.setup_logging()\n",
    "logging.info(\"필수 라이브러리 및 data_utils 모듈 임포트, 로깅 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e979d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직종 구분 선택 -> 사이트마다 다르게 설정할 것\n",
    "job_category = \"total\" # \"total\" # \"backend\" # \"frontend\"\n",
    "\n",
    "if job_category == \"total\":\n",
    "    job_category_url = \"?\"\n",
    "elif job_category == \"backend\":\n",
    "    job_category_url = \"?job_ids=872\"\n",
    "elif job_category == \"frontend\":\n",
    "    job_category_url = \"?job_ids=669\"\n",
    "else:\n",
    "    job_category_url = \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크래핑 설정\n",
    "headers = data_utils.DEFAULT_HEADERS\n",
    "site_name = 'wanted'\n",
    "\n",
    "base_url = f\"https://www.wanted.co.kr/api/chaos/navigation/v1/results{job_category_url}\"\n",
    "\n",
    "scraped_data = []\n",
    "offset_no = 0\n",
    "\n",
    "# limit = 페이지 당 가져올 공고 수\n",
    "# max_offset = 최대 가져올 공고 수\n",
    "limit = 20\n",
    "max_offset = 3000\n",
    "\n",
    "# 기본 URL 파라미터\n",
    "base_params = {\n",
    "    'job_group_id': 518, # 개발 직군 전체\n",
    "    'country': 'kr',\n",
    "    'job_sort': 'job.latest_order', # 최신순 정렬\n",
    "    'years': -1, # 경력 무관\n",
    "    'locations': 'all', # 전체 지역\n",
    "    'limit': limit,\n",
    "    # 'offset'은 루프 내에서 설정됨\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"스크래핑 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c33810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상세 정보 추출 함수 정의\n",
    "def get_job_details(job_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    주어진 job_id에 해당하는 상세 페이지 API에서 직무 상세 설명을 추출합니다.\n",
    "    API 엔드포인트: https://www.wanted.co.kr/api/chaos/jobs/v4/{job_id}/details\n",
    "    추출 필드: data.job.detail.main_tasks, data.job.detail.requirements, data.job.detail.preferred_points\n",
    "    \"\"\"\n",
    "\n",
    "    api_url = f\"https://www.wanted.co.kr/api/chaos/jobs/v4/{job_id}/details\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()  # HTTP 오류 발생 시 예외 발생\n",
    "        data = response.json()\n",
    "\n",
    "        if 'data' in data and 'job' in data['data'] and 'detail' in data['data']['job']:\n",
    "            detail = data['data']['job']['detail']\n",
    "            main_tasks = detail.get('main_tasks', '')\n",
    "            requirements = detail.get('requirements', '')\n",
    "            preferred_points = detail.get('preferred_points', '')\n",
    "\n",
    "            # 각 항목을 줄바꿈 두 번으로 연결하여 상세 정보 생성\n",
    "            detail_text = \"\\n\\n\".join(filter(None, [main_tasks, requirements, preferred_points])).strip()\n",
    "            return detail_text\n",
    "        else:\n",
    "            logging.warning(f\"Detail section not found in API response for job ID {job_id}. URL: {api_url}. Response: {data}\")\n",
    "            return \"상세 내용 없음 (API 응답에 Detail 섹션 부재)\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching details from API for job ID {job_id}: {e}. URL: {api_url}\")\n",
    "        return f\"상세 내용 로드 실패 (API 요청 오류: {e})\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing API response for job ID {job_id}: {e}\", exc_info=True)\n",
    "        return f\"상세 내용 파싱 실패 (API 응답 오류: {e})\"\n",
    "\n",
    "# # --- 사용 예시 (테스트용, 실제 스크래핑 루프에서는 호출됨) ---\n",
    "# test_job_id = 260432 # 테스트하려는 실제 공고 ID 입력\n",
    "# test_details = get_job_details(test_job_id)\n",
    "# print(f\"--- 테스트 결과 (Job ID: {test_job_id}) ---\")\n",
    "# print(test_details)\n",
    "# print(type(get_job_details(test_job_id)))\n",
    "# print(\"--- 테스트 종료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7be5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크래핑 루프\n",
    "logging.info(\"스크래핑 루프 시작...\")\n",
    "\n",
    "while offset_no < max_offset:\n",
    "    # 현재 offset을 포함한 파라미터 설정\n",
    "    current_params = base_params.copy()\n",
    "    current_params['offset'] = offset_no\n",
    "\n",
    "    logging.info(f\"--- Offset {offset_no} 데이터 가져오는 중 ---\")\n",
    "\n",
    "    try:\n",
    "        # API 요청 (타임아웃 20초)\n",
    "        response = requests.get(base_url, params=current_params, headers=headers, timeout=20)\n",
    "        response.raise_for_status() # 오류 발생 시 예외 발생\n",
    "        json_data = response.json()\n",
    "\n",
    "        if 'data' in json_data and json_data['data']:\n",
    "            jobs = json_data['data']\n",
    "            logging.info(f\"Offset {offset_no}: {len(jobs)}개 공고 발견.\")\n",
    "            #\n",
    "\n",
    "            for job in jobs:\n",
    "                try:\n",
    "                    company = job.get('company', {})\n",
    "                    company_id = job.get('id')\n",
    "                    company_name = company.get('name')\n",
    "                    position = job.get('position')\n",
    "                    # address = job.get('address', {}) # address 정보 가져오기\n",
    "                    # location = address.get('location') # location 정보 추출\n",
    "                    # district = address.get('district') # district 정보 추출\n",
    "    \n",
    "                    if not company_id:\n",
    "                        logging.warning(f\"ID가 없는 공고 건너<0xEB><0x9B><0x9C>. Position: {position}, Company: {company_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 상세 정보 가져오기 (정의된 함수 호출)\n",
    "                    logging.info(f\"  - 상세 정보 가져오는 중 (현재 offset: {offset_no}, ID: {company_id}, Company: {company_name})\") # 너무 길면 잘라서 로깅\n",
    "                    skill = get_job_details(company_id)\n",
    "                    # 각 상세 페이지 요청 사이에 1초 지연 (서버 부하 감소)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    # --- skill 값 필터링 (문자열 등 정규화) ---\n",
    "                    filtered_skill = data_utils.filter_skill_data(skill) # 필터링 함수 호출\n",
    "                    # --- skill 값 필터링 완료 ---\n",
    "\n",
    "                    job_info = {\n",
    "                        'company': company_name,\n",
    "                        'position': position,\n",
    "                        'skill': filtered_skill,\n",
    "                        # 'job_id': company_id, # 참고용으로 ID도 저장\n",
    "                        # 'location': location, # location 정보 추가\n",
    "                        # 'district': district # district 정보 추가\n",
    "                    }\n",
    "                    scraped_data.append(job_info)\n",
    "    \n",
    "                except Exception as e:\n",
    "                    # 개별 공고 처리 중 오류 발생 시 로깅하고 계속 진행\n",
    "                    logging.error(f\"개별 공고 처리 중 오류 발생 (ID: {job.get('id')}): {e}\", exc_info=True) # 상세 오류 스택 출력\n",
    "                    continue\n",
    "\n",
    "            # 다음 페이지로 이동\n",
    "            offset_no += limit\n",
    "            # API 목록 요청 사이에 1.5초 지연\n",
    "            logging.info(f\"Offset {offset_no - limit} 처리 완료. 1.5초 대기...\")\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        else:\n",
    "            # API 응답에 'data'가 없거나 비어있는 경우\n",
    "            logging.info(f\"Offset {offset_no}에서 더 이상 데이터가 없거나 API 응답 구조 변경됨. 스크래핑 중단.\")\n",
    "            break\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.error(f\"API 요청 시간 초과 (Offset: {offset_no}). 10초 후 재시도...\")\n",
    "        time.sleep(10)\n",
    "        # continue # 현재 offset 재시도 위해 offset_no 증가시키지 않음 (루프 시작 시 재시도됨)\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logging.error(f\"HTTP 오류 발생 (Offset: {offset_no}): {e.response.status_code} - {e.response.reason}\")\n",
    "        if e.response.status_code == 429: # Too Many Requests\n",
    "            logging.warning(\"요청 속도 제한(429) 감지됨. 60초 대기 후 재시도...\")\n",
    "            time.sleep(60)\n",
    "            # continue # 재시도\n",
    "        elif 400 <= e.response.status_code < 500:\n",
    "            logging.error(\"클라이언트 오류 발생. 요청 파라미터나 헤더 확인 필요. 스크래핑 중단.\")\n",
    "            break\n",
    "        elif e.response.status_code >= 500:\n",
    "            logging.warning(\"서버 오류 발생. 30초 대기 후 재시도...\")\n",
    "            time.sleep(30)\n",
    "            # continue # 재시도\n",
    "        else:\n",
    "            logging.error(\"알 수 없는 HTTP 오류. 스크래핑 중단.\")\n",
    "            break\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"API 요청 중 오류 발생 (Offset: {offset_no}): {e}\")\n",
    "        logging.info(\"네트워크 연결 확인 후 20초 대기...\")\n",
    "        time.sleep(20)\n",
    "        # continue # 재시도\n",
    "\n",
    "    except Exception as e:\n",
    "        # 기타 예외 처리 (JSON 파싱 오류 등)\n",
    "        logging.error(f\"스크래핑 루프 중 예기치 않은 오류 발생 (Offset: {offset_no}): {e}\", exc_info=True)\n",
    "        logging.info(\"오류로 인해 스크래핑 중단.\")\n",
    "        break\n",
    "\n",
    "logging.info(f\"스크래핑 루프 종료. 총 {len(scraped_data)}개의 공고 데이터 수집 완료.\")\n",
    "\n",
    "# # (선택적) 수집된 데이터 개수 확인\n",
    "# print(f\"Total jobs collected: {len(scraped_data)}\")\n",
    "# # (선택적) 첫번째 데이터 샘플 확인\n",
    "# if scraped_data:\n",
    "#     print(\"\\nSample Data:\")\n",
    "#     print(scraped_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_utils 모듈의 save_data_to_csv 함수를 사용하여 데이터를 저장합니다.\n",
    "data_folder = 'data'\n",
    "filename = f'data_{site_name}_{job_category}.csv'\n",
    "\n",
    "saved_filepath = data_utils.save_data_to_csv(scraped_data, filename, folder=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246be554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 CSV 파일을 읽어와 DataFrame으로 출력 (확인용)\n",
    "data_utils.load_data_from_csv(saved_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
